The following code below shows the forward propagation procedure for the layers.

For the Dense Layer: 
\begin{minted}[
frame=lines,
breaklines=true
]{vb.net}
    Public Function f(ByVal X As Tensor) As Matrix Implements Layer(Of Matrix).f 'Shape of X is (_units_prev, m)
        x_in = X
        z = Matrix.matmul(w, X) + b
        a = act.f(z)
        Return a
    End Function 'Returns the output of the layer using the inputs fed into the layer
\end{minted}

For the conv Layer:
\begin{minted}[
frame=lines,
breaklines=true
]{vb.net}
    Public Function f(x As Tensor) As Volume Implements Layer(Of Volume).f
        'x is the input into the layer
        x_in = x
        z = Volume.conv2d(x_in, filter, stridesx, stridesy, padding) + b
        a = Volume.op(AddressOf act.f, z)
        Return a
    End Function 'Returns the output of the layer using the inputs fed into the layer
\end{minted}

Finally, the following code below shows the back-propagation procedure for the layers.

For the Dense Layer: 
\begin{minted}[
frame=lines,
breaklines=true
]{vb.net}
    Public Overridable Overloads Function update(ByVal l_r As Decimal, ByVal prev_delta As Tensor, ByVal ParamArray param() As Tensor) As Tensor Implements Layer(Of Matrix).update
        Dim grads As IEnumerable(Of Matrix) = gradient(prev_delta, param) 'grads returns (dw, db)
        w -= l_r * grads(0) : b -= l_r * grads(1) 'Parameters are being updated
        Return grads(1) 'Function returns db, as will be needed for the next-layers update for back-prop
    End Function 'Function updates the parameters using prev_delta and  param

    Public Overloads Function update(ByVal l_r As Decimal, ByVal prev_delta As Tensor) As Tensor Implements Layer(Of Matrix).update
        'This update function is only used by the Last layer in the Net if it is a dense net
        Dim grads As IEnumerable(Of Matrix) = gradient(prev_delta) 'gradient function returns (dw, db)
        w -= l_r * grads(0) : b -= l_r * grads(1) 'Parameters are being updated
        Return grads(1) 'Function returns db, as will be needed for the next-layers update for back-prop
    End Function
\end{minted}

For the conv Layer:
\begin{minted}[
frame=lines,
breaklines=true
]{vb.net}
    Public Function update(l_r As Decimal, _prev_dz As Tensor, ParamArray param() As Tensor) As Tensor Implements Layer(Of Volume).update
        'param should be empty as convolution doesn't require previous weights for finding updates.
        Dim dfilter As New List(Of Matrix)
        Dim dz As Volume = _prev_dz.clone
        Dim act_d As Volume = Volume.op(AddressOf act.d, z)
        Dim dx As New List(Of Matrix)

        'Following code finds dfilter, using _prev_dz
        For filter_channel As Integer = 0 To filter.values.Count - 1
            Dim temp As New Volume(kernely, kernelx, 1)
            Dim i As Integer = 0
            For Each kernel_window In Volume.subvolume(x_in, kernelx, kernely, stridesx, stridesy)
                temp += kernel_window * dz.item(Math.Truncate(i / dz.shape.Item2) + 1, (i Mod dz.shape.Item2) + 1, filter_channel) * act_d.item(Math.Truncate(i / dz.shape.Item2) + 1, (i Mod dz.shape.Item2) + 1, filter_channel)
                i += 1
            Next
            dfilter.Add(temp.mean(2))
        Next

        'The Following code finds dx using dfilter
        For dx_channel As Integer = 0 To x_in.values.Count - 1
            Dim dx_channel_sum As New Volume(x_in.shape.Item1, x_in.shape.Item2, 1)
            For f As Integer = 0 To filter.values.Count - 1
                Dim k As Integer = 0
                For i As Integer = 0 To x_in.shape.Item1 - kernely Step stridesy
                    For j As Integer = 0 To x_in.shape.Item2 - kernelx Step stridesx
                        dx_channel_sum.split(i + 1, i + kernely, j + 1, j + kernelx, 0) = filter.values(f) * dz.item(Math.Truncate(k / dz.shape.Item2) + 1, (k Mod dz.shape.Item2) + 1, f)
                        k += 1
                    Next
                Next
                dx.Add(dx_channel_sum.mean(2))
            Next
        Next

        filter -= New Volume(dfilter) * l_r
        b -= Volume.op(AddressOf Matrix.sum, _prev_dz) * l_r
        Return New Volume(dx)
    End Function
\end{minted}