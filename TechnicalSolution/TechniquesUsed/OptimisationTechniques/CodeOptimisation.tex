The following algorithms below were used to train the net and they are, ADAM, RMS, Momentum and the standard back-propagation algorithm.

The following code below is the code that was implemented for the standard backpropagation algorithm.

\begin{minted}[
frame=lines,
breaklines=true
]{vb.net}

    Public Overrides Function run(ByVal learning_rate As Decimal, ByVal printLoss As Boolean, ByVal batchSize As Integer, ByVal ParamArray param() As Decimal) As List(Of Tensor)
        Dim batches As List(Of IEnumerable(Of Tuple(Of Tensor, Tensor))) = MyBase.splitdata(batchSize) 'This line splits the data into the different batchsizes
        'Following code find the average cost for this particulat mini-batch 

        Dim pred As Tensor
        Dim errors As New List(Of Matrix)

        For Each batch In batches 'Looping over every batch
            'The following code wil now find the average dirivative w.r.t the output of the net for this particulat mini-batch
            For Each vector In batch
                pred = model.predict(vector.Item1)
                errors.Add(model.loss.d(pred, vector.Item2) * model.netLayers.Peek.parameters.Last)
            Next
            Dim deltas As New Stack(Of Tensor)
            deltas.Push((New Volume(errors) / batch.Count).mean(2))

            'Following code updates each layer using deltas
            model.netLayers.Peek.update(learning_rate, deltas.Peek)
            For layer As Integer = 1 To model.netLayers.Count - 1
                deltas.Push(model.netLayers(layer).update(learning_rate, deltas.Peek, model.netLayers(layer - 1).parameters(0)))
            Next
        Next

        losses.Add(calculateCost(dataxy)) 'loss is calulcated using the new updates
        If printLoss Then
            Console.WriteLine("Error for epoch {0} is: ", iterations)
            losses.Last.print()
        End If
        iterations += 1
        Return losses
    End Function 'Function applies mini-batch gradient descent to train the network

\end{minted}

The following code below is the code that was implemented for the Momentum Optimisation algorithm.

\begin{minted}[
frame=lines,
breaklines=true
]{vb.net}
    Public Overrides Function run(ByVal l_r As Decimal, ByVal printLoss As Boolean, ByVal batchSize As Integer, ParamArray param() As Decimal) As List(Of Tensor)
        If param.Count <> 1 Then
            Throw New System.Exception("Momentum requires 1 parameters for training")
        End If

        Dim batches As List(Of IEnumerable(Of Tuple(Of Tensor, Tensor))) = MyBase.splitdata(batchSize) 'This line splits the data into the different batchsizes

        For Each batch In batches 'Looping through each batch, as we are doing mini-batch gradient descent
            Dim d As Tuple(Of List(Of Matrix), List(Of Matrix)) = MyBase.calculateGradients(batch)
            Dim dw As List(Of Matrix) = d.Item1 : Dim db As List(Of Matrix) = d.Item2 'This line retreives the gradients for w & b

            'Following code applies the momentum optimiser technique to the network
            For layer As Integer = 0 To model.netLayers.Count - 1
                v_dw(layer) = (v_dw(layer) * param(0) + (1 - param(0)) * dw(layer))
                v_db(layer) = (v_db(layer) * param(0) + (1 - param(0)) * db(layer))
                model.netLayers(layer).deltaUpdate(-l_r * v_dw(layer), -l_r * v_db(layer))
            Next
        Next

        'Following code will store the new loss after the updates have been done
        losses.Add(calculateCost(dataxy))
        If printLoss Then
            Console.WriteLine("Error for epoch {0} is: ", iterations)
            losses.Last.print()
        End If
        iterations += 1
        Return losses

    End Function

\end{minted}

The following code below is the code that was implemented for the RMS Optimisation algorithm.

\begin{minted}[
frame=lines,
breaklines=true
]{vb.net}
    Public Overrides Function run(ByVal l_r As Decimal, ByVal printLoss As Boolean, ByVal batchSize As Integer, ParamArray param() As Decimal) As List(Of Tensor)
        If param.Count <> 1 Then
            Throw New System.Exception("RMS requires 1 parameters for training")
        End If

        Dim batches As List(Of IEnumerable(Of Tuple(Of Tensor, Tensor))) = MyBase.splitdata(batchSize) 'This line splits the data into the different batchsizes

        For Each batch In batches 'Looping through each batch, as we are doing mini-batch gradient descent
            Dim d As Tuple(Of List(Of Matrix), List(Of Matrix)) = calculateGradients(batch)
            Dim dw As List(Of Matrix) = d.Item1 : Dim db As List(Of Matrix) = d.Item2
            'Following code applies the RMS optimisation technique to the network
            For layer As Integer = 0 To model.netLayers.Count - 1
                s_dw(layer) = s_dw(layer) * param(0) + (1 - param(0)) * dw(layer) * dw(layer)
                s_db(layer) = s_db(layer) * param(0) + (1 - param(0)) * db(layer) * db(layer)
                model.netLayers(layer).deltaUpdate(-l_r * dw(layer) * (1 / (Matrix.op(Function(x) Math.Sqrt(x), s_dw(layer)) + 0.000001)), -l_r * db(layer) * (1 / (Matrix.op(Function(x) Math.Sqrt(x), s_db(layer)) + 0.000001)))
            Next
        Next

        'Following code will store the new loss after the updates have been done
        losses.Add(calculateCost(dataxy))
        If printLoss Then
            Console.WriteLine("Error for epoch {0} is: ", iterations)
            losses.Last.print()
        End If
        iterations += 1
        Return losses
    End Function
\end{minted}

The following code below is the code that was implemented for the ADAM Optimisation algorithm.
\begin{minted}[
frame=lines,
breaklines=true
]{vb.net}
    Public Overrides Function run(ByVal l_r As Decimal, ByVal printLoss As Boolean, ByVal batchSize As Integer, ParamArray param() As Decimal) As List(Of Tensor)
        If param.Count <> 2 Then
            Throw New System.Exception("Adam requires 2 parameters for training")
        End If


        Dim batches As List(Of IEnumerable(Of Tuple(Of Tensor, Tensor))) = MyBase.splitdata(batchSize) 'This line splits the data into the different batchsizes
        Dim decay_term As Decimal = Math.Sqrt(1 - Math.Pow(param(1), iterations)) / (1 - Math.Pow(param(0), iterations) + 0.000001) 'decay term is being set for this particular iteration

        For Each batch In batches 'Looping through each batch, as we are doing mini-batch gradient descent
            Dim d As Tuple(Of List(Of Matrix), List(Of Matrix)) = calculateGradients(batch)
            Dim dw As List(Of Matrix) = d.Item1 : Dim db As List(Of Matrix) = d.Item2

            'The following code applies Adam Optimization technique to the network
            For layer As Integer = 0 To model.netLayers.Count - 1
                s_dw(layer) = s_dw(layer) * param(1) + (1 - param(1)) * dw(layer) * dw(layer)
                s_db(layer) = s_db(layer) * param(1) + (1 - param(1)) * db(layer) * db(layer)
                v_dw(layer) = (v_dw(layer) * param(0) + (1 - param(0)) * dw(layer))
                v_db(layer) = (v_db(layer) * param(0) + (1 - param(0)) * db(layer))
                model.netLayers(layer).deltaUpdate(-l_r * dw(layer) * (1 / (Matrix.op(Function(x) Math.Sqrt(x), s_dw(layer)) + 0.000001)), -l_r * db(layer) * (1 / (Matrix.op(Function(x) Math.Sqrt(x), s_db(layer)) + 0.000001)))
                model.netLayers(layer).deltaUpdate(-l_r * v_dw(layer) * (1 / (Matrix.op(Function(x) Math.Sqrt(x), s_dw(layer)) + 0.00001)) * decay_term, -l_r * v_db(layer) * (1 / (Matrix.op(Function(x) Math.Sqrt(x), s_db(layer)) + 0.00001)) * decay_term)
            Next
        Next

        'Following code will store the new loss after the updates have been done
        losses.Add(calculateCost(dataxy))
        If printLoss Then
            Console.WriteLine("Error for epoch {0} is: ", iterations)
            losses.Last.print()
        End If
        iterations += 1
        Return losses

    End Function
\end{minted}